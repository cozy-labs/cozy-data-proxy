Tests
=====

Dependencies
------------

Dependencies are managed with [Yarn](https://yarnpkg.com/), so you'll
have to [install it](https://yarnpkg.com/en/docs/install) first.

Then, to install dev dependencies:

```bash
yarn
```

We use [`mocha`][1] for testing cozy-desktop, the options are in
[`test/mocha.opt`][2].

Integration tests and some unit tests require that you have a Cozy stack up.
It's also expected that you have an instance registered for
`test.cozy-desktop.tools:8080` with the
[test passphrase](../../test/support/helpers/passphrase.js).
You can start a cozy-stack via the provided docker-compose file:

```
docker-compose up
```

See [requirements](./requirements.md) for details on how to setup docker and docker-compose.

Unit tests
----------

For testing a class in isolation, method per method:

```bash
yarn test:unit
```


Integration tests
-----------------

:warning: **Important**: the integration tests remove all the files and folders
on the Cozy! We recommend using the default repository with
`COZY_DESKTOP_DIR=tmp`.

You can run the integration suite to test the communication between
cozy-desktop and a remote cozy stack:

```bash
COZY_DESKTOP_DIR=tmp yarn test:integration
```


Scenarios
---------

Recently we have been writing more and more scenarios as plain data in
`./cli/test/scenarios/*/scenario.js` files. Those can then be used to *capture*
local and/or remote events generated by those scenarios actions (by running the
`yarn capture` script from the `./cli/` directory, see `yarn capture -- --help`
for more information). The captured events are stored in the corresponding
`./cli/test/scenarios/*/{local|remote}/` subdirectory. Finally we can use those
captures as fixtures in tests (see `cli/test/integration/scenarios.js`). The
main benefit is that both local and remote events can come in remote order, so
using versioned test input makes the tests repetable/reliable and allows us to
try different event sequences.

- TODO: Refactor `cli/dev/capture*` and `cli/test/helpers/scenarios.js`
- TODO: The local/remote wording is confusing. Use source/target instead?
- TODO: Don't name local captures after the platform since they usually are not
  platform-specific.
- TODO: Use captures instead of real actions when run from the remote side (so
  we can test different event sequences)
- TODO: Use captures when run from the local side with stopped client (so tests
  don't fail randomly)
- TODO: Run scenarios with Cozy offline first, then back
- TODO: Enable and fix the last failing scenarios
- TODO: Eventually stop asserting the whole chain in every scenario to make the
  build faster
- TODO: Eventually check scenario data structure with Flow (so one doesn't
  believe everything is green while nothing is actually tested)
- TODO: Eventually find a way to test a few loop effects

Options
-------

It's possible to launch unit and integration tests:

```bash
COZY_DESKTOP_DIR=tmp yarn test
```

To run a specific set of tests (here testing pouch)

```bash
yarn mocha test/unit/pouch.js
```

For more logs you can activate debug logs:

```bash
DEBUG=true COZY_DESKTOP_DIR=tmp yarn test
```


Coverage
--------

You can enable coverage metrics for any npm command with the
[`coverage.sh`][3] script.

Examples:

```bash
./scripts/coverage.sh yarn test
./scripts/coverage.sh yarn test-unit
```

Please note that code coverage is only measured for unit tests.
Integration tests have another purpose, so they are deliberately excluded,
even when running `./scripts/coverage.sh yarn test-integration`
explicitely.

Please also note that we don't measure coverage on the GUI for now.

Implementation details:

1. `yarn test-unit-coverage` wraps the `mocha` command with
   [nyc][3] to compile the code with [babel-plugin-istanbul][3].
the [appropriate option][3].
2. `babel-plugin-istanbul` inserts instrumentation code when compiling from
   EcmaScript to JavaScript
3. The mocha tests are run and generate an lcov-style report (including
   HTML output)
4. Finally, when run on the CI, we [tell Travis](../.travis.yml) to upload the report to the
   [Codecov][5] service.


Property based testing
----------------------

In theory, property based testing is a way to generalize some unit tests.
Here, we are using them more as fuzzing / intergration tests.

The property based testing is not currently runned on CI: it would need some
work to do so. Currently, it is mainly aimed at running manually by an
experienced developer to find bugs that manual tests haven't found.

We have two types of property based testing:

- local_watcher, where we simulate events of a disk, and at the end we check
  that PouchDB has all the information about the files and directory in the
  synchronized path. It's called local_watcher because it mostly tests the
  local watcher, but it also touches other classes like Prep and Merge.

- two_clients, where we start a cozy-stack, create an instance, and run 2
  cozy-desktop on this instance. Then, we doing things like creating files on
  both clients, and at the end, we check that the two clients has the same data
  as the stack (in CouchDB).

For both, we have separated the generation of a test case from running it, and
we have no shrinking strategy (no good JS library to do that). The generated
test case are in JSON format, and it's possible to write manually a test case
to exibit particular behavior.

You can generate a test with this command:

```sh
$ ./test/generate_property_json.js local_watcher | jq . > test/property/local_watcher/generated.json
```

And then, you can run it with:

```sh
$ COZY_FS_WATCHER=atom yarn test:property --grep generated
```

If you want to run several tests for finding new bugs, there is also the
`./test/mass_property_tests.sh` script.


[1]:  https://mochajs.org/
[2]:  ../test/mocha.opts
[3]: https://github.com/istanbuljs/nyc
[4]: https://github.com/istanbuljs/babel-plugin-istanbul
[5]: https://codecov.io/gh/cozy-labs/cozy-desktop
